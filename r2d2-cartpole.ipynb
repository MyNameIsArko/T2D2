{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:46:31.194271Z",
     "start_time": "2024-05-14T09:46:28.404470Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "# from tensorboardX import SummaryWriter\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7809a363e30f2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'CartPole-v1'\n",
    "\n",
    "lr = 1e-4  \n",
    "eps = 1e-3  \n",
    "grad_norm = 40  \n",
    "batch_size = 32  \n",
    "learning_starts = 1000  \n",
    "save_interval = 100  \n",
    "target_net_update_interval = 500  \n",
    "gamma = 0.99  \n",
    "prio_exponent = 0.9  \n",
    "importance_sampling_exponent = 0.6  \n",
    "\n",
    "training_steps = 10000  \n",
    "buffer_capacity = 50000  \n",
    "max_episode_steps = 200  \n",
    "actor_update_interval = 100  \n",
    "block_length = 50  \n",
    "\n",
    "num_actors = 8  \n",
    "base_eps = 0.4  \n",
    "alpha = 7  \n",
    "log_interval = 10  \n",
    "\n",
    "# sequence setting\n",
    "burn_in_steps = 10  \n",
    "learning_steps = 10  \n",
    "forward_steps = 5  \n",
    "seq_len = burn_in_steps + learning_steps + forward_steps\n",
    "\n",
    "# network setting\n",
    "hidden_dim = 256  \n",
    "\n",
    "render = False  \n",
    "save_plot = True  \n",
    "test_epsilon = 0.001  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36cbf333688b51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityTree:\n",
    "    def __init__(self, capacity, prio_exponent, is_exponent):\n",
    "        self.num_layers = 1\n",
    "        while capacity > 2**(self.num_layers - 1):\n",
    "            self.num_layers += 1\n",
    "        \n",
    "        self.ptree = np.zeros(2**self.num_layers - 1, dtype=np.float64)\n",
    "        \n",
    "        self.prio_exponent = prio_exponent\n",
    "        self.is_exponent = is_exponent\n",
    "    \n",
    "    def update(self, idxes: np.ndarray, td_error: np.ndarray) -> None:\n",
    "        priorities = td_error ** self.prio_exponent\n",
    "        \n",
    "        idxes = idxes + 2**(self.num_layers - 1) - 1\n",
    "        self.ptree[idxes] = priorities\n",
    "        \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            idxes = (idxes - 1) // 2\n",
    "            idxes = np.unique(idxes)\n",
    "            self.ptree[idxes] = self.ptree[2 * idxes + 1] + self.ptree[2 * idxes + 2]\n",
    "    \n",
    "    def sample(self, num_samples: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        p_sum = self.ptree[0]\n",
    "        interval = p_sum / num_samples\n",
    "        \n",
    "        prefixsums = np.arange(0, p_sum, interval, dtype=np.float64) + np.random.uniform(0, interval, num_samples)\n",
    "        \n",
    "        idxes = np.zeros(num_samples, dtype=np.int64)\n",
    "        for _ in range(self.num_layers-1):\n",
    "            nodes = self.ptree[2 * idxes + 1]\n",
    "            idxes = np.where(prefixsums < nodes, 2 * idxes + 1, 2 * idxes + 2)\n",
    "            prefixsums = np.where(idxes%2 == 0, prefixsums - self.ptree[idxes-1], prefixsums)\n",
    "        \n",
    "        priorities = self.ptree[idxes]\n",
    "        min_p = np.min(priorities)\n",
    "        is_weigths = np.power(priorities/min_p, -self.is_exponent)\n",
    "        \n",
    "        idxes -= 2**(self.num_layers - 1) - 1\n",
    "        \n",
    "        return idxes, is_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d92553642b4f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Block:\n",
    "    obs: np.array\n",
    "    last_action: np.array\n",
    "    last_reward: np.array\n",
    "    action: np.array\n",
    "    n_step_reward: np.array\n",
    "    gamma: np.array\n",
    "    hidden: np.array\n",
    "    num_sequences: int\n",
    "    burn_in_steps: np.array\n",
    "    learning_steps: np.array\n",
    "    forward_steps: np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6533684a57fe978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, sample_queue_list, batch_queue, priority_queue, alpha=prio_exponent, beta=importance_sampling_exponent):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.sequence_len = learning_steps\n",
    "        self.num_sequences = buffer_capacity // self.sequence_len\n",
    "        self.block_len = block_length\n",
    "        self.num_blocks = self.buffer_capacity // self.block_len\n",
    "        self.seq_pre_block = self.block_len // self.sequence_len\n",
    "        \n",
    "        self.block_ptr = 0\n",
    "        \n",
    "        self.priority_tree = PriorityTree(self.num_sequences, alpha, beta)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.env_steps = 0\n",
    "        \n",
    "        self.num_episodes = 0\n",
    "        self.episode_reward = 0\n",
    "        \n",
    "        self.training_steps = 0\n",
    "        self.last_training_steps = 0\n",
    "        self.sum_loss = 0\n",
    "        \n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        self.size = 0\n",
    "        self.last_size = 0\n",
    "        \n",
    "        self.buffer = [None] * self.num_blocks\n",
    "        \n",
    "        self.sample_queue_list = sample_queue_list\n",
    "        self.batch_queue = batch_queue\n",
    "        self.priority_queue = priority_queue\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def run(self):\n",
    "        background_thread = threading.Thread(target=self.add_data, daemon=True)\n",
    "        background_thread.start()\n",
    "        \n",
    "        background_thread = threading.Thread(target=self.prepare_data, daemon=True)\n",
    "        background_thread.start()\n",
    "        \n",
    "        background_thread = threading.Thread(target=self.update_data, daemon=True)\n",
    "        background_thread.start()\n",
    "\n",
    "        writer_thread = threading.Thread(target=self.write_stats, daemon=True)\n",
    "        writer_thread.start()\n",
    "        \n",
    "        while True:\n",
    "            print(f'Buffer size: {self.size}')\n",
    "            print(f'Buffer update speed: {(self.size - self.last_size) / log_interval}/s')\n",
    "            self.last_size = self.size\n",
    "            print(f'Number of environment steps: {self.env_steps}')\n",
    "            if self.num_episodes != 0:\n",
    "                print(f'Average episode return: {self.episode_reward / self.num_episodes:.4f}')\n",
    "                self.episode_reward = 0\n",
    "                self.num_episodes = 0\n",
    "            print(f'Number of training steps: {self.training_steps}')\n",
    "            print(f'Training speed: {(self.training_steps - self.last_training_steps) / log_interval}/s')\n",
    "            if self.training_steps != self.last_training_steps:\n",
    "                print(f'Loss: {self.sum_loss/(self.training_steps-self.last_training_steps):.4f}')\n",
    "                self.last_training_steps = self.training_steps\n",
    "                self.sum_loss = 0\n",
    "            self.last_env_steps = self.env_steps\n",
    "            print()\n",
    "            \n",
    "            if self.training_steps == training_steps:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(log_interval)\n",
    "    \n",
    "    def write_stats(self):\n",
    "        # writer = SummaryWriter(f'logs/r2d2__{game_name}')\n",
    "        os.makedirs('logs/r2d2', exist_ok=True)\n",
    "        while True:\n",
    "            # writer.add_scalar('buffer/size', self.size, self.env_steps)\n",
    "            with open('logs/r2d2/buffer_size.txt', 'a') as f:\n",
    "                f.write(f'{self.size} {self.env_steps}\\n')\n",
    "            \n",
    "            # writer.add_scalar('buffer/update_speed', (self.size - self.last_size) / log_interval, self.env_steps)\n",
    "            with open('logs/r2d2/buffer_update_speed.txt', 'a') as f:\n",
    "                f.write(f'{(self.size - self.last_size) / log_interval} {self.env_steps}\\n')\n",
    "            \n",
    "            # writer.add_scalar('environment/steps', self.env_steps, self.env_steps)\n",
    "            with open('logs/r2d2/environment_steps.txt', 'a') as f:\n",
    "                f.write(f'{self.env_steps}\\n')\n",
    "\n",
    "            if self.num_episodes != 0:\n",
    "                # writer.add_scalar('episode/return', self.episode_reward / self.num_episodes, self.env_steps)\n",
    "                with open('logs/r2d2/episode_return.txt', 'a') as f:\n",
    "                    f.write(f'{self.episode_reward / self.num_episodes} {self.env_steps}\\n')\n",
    "\n",
    "            # writer.add_scalar('training/steps', self.training_steps, self.env_steps)\n",
    "            with open('logs/r2d2/training_steps.txt', 'a') as f:\n",
    "                f.write(f'{self.training_steps} {self.env_steps}\\n')\n",
    "\n",
    "            # writer.add_scalar('training/speed', (self.training_steps - self.last_training_steps) / log_interval, self.env_steps)\n",
    "            with open('logs/r2d2/training_speed.txt', 'a') as f:\n",
    "                f.write(f'{(self.training_steps - self.last_training_steps) / log_interval} {self.env_steps}\\n')\n",
    "\n",
    "            if self.training_steps != self.last_training_steps:\n",
    "                # writer.add_scalar('training/loss', self.sum_loss/(self.training_steps-self.last_training_steps), self.env_steps)\n",
    "                with open('logs/r2d2/training_loss.txt', 'a') as f:\n",
    "                    f.write(f'{self.sum_loss/(self.training_steps-self.last_training_steps)} {self.env_steps}\\n')\n",
    "\n",
    "            time.sleep(log_interval)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        while self.size < learning_starts:\n",
    "            time.sleep(1)\n",
    "        \n",
    "        while True:\n",
    "            if not self.batch_queue.full():\n",
    "                data = self.sample_batch()\n",
    "                self.batch_queue.put(data)\n",
    "            else:\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "    def add_data(self):\n",
    "        while True:\n",
    "            for sample_queue in self.sample_queue_list:\n",
    "                if not sample_queue.empty():\n",
    "                    data = sample_queue.get_nowait()\n",
    "                    self.add(*data)\n",
    "    \n",
    "    def update_data(self):\n",
    "        while True:\n",
    "            if not self.priority_queue.empty():\n",
    "                data = self.priority_queue.get_nowait()\n",
    "                self.update_priorities(*data)\n",
    "            else:\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "    def add(self, block: Block, priority: np.array, episode_reward: float):\n",
    "        with self.lock:\n",
    "            idxes = np.arange(self.block_ptr * self.seq_pre_block, (self.block_ptr + 1) * self.seq_pre_block, dtype=np.int64)\n",
    "            \n",
    "            self.priority_tree.update(idxes, priority)\n",
    "            \n",
    "            if self.buffer[self.block_ptr] is not None:\n",
    "                self.size -= np.sum(self.buffer[self.block_ptr].learning_steps).item()\n",
    "            \n",
    "            self.size += np.sum(block.learning_steps).item()\n",
    "            \n",
    "            self.buffer[self.block_ptr] = block\n",
    "            \n",
    "            self.env_steps += np.sum(block.learning_steps, dtype=np.int32)\n",
    "            \n",
    "            self.block_ptr = (self.block_ptr + 1) % self.num_blocks\n",
    "            \n",
    "            if episode_reward:\n",
    "                self.episode_reward += episode_reward\n",
    "                self.num_episodes += 1\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        batch_obs, batch_last_action, batch_last_reward, batch_hidden, batch_action, batch_reward, batch_gamma = [], [], [], [], [], [], []\n",
    "        burn_in_steps, learning_steps, forward_steps = [], [], []\n",
    "\n",
    "        with self.lock:\n",
    "\n",
    "            idxes, is_weights = self.priority_tree.sample(self.batch_size)\n",
    "\n",
    "            block_idxes = idxes // self.seq_pre_block\n",
    "            sequence_idxes = idxes % self.seq_pre_block\n",
    "\n",
    "            for block_idx, sequence_idx  in zip(block_idxes, sequence_idxes):\n",
    "\n",
    "                block = self.buffer[block_idx]\n",
    "\n",
    "                assert sequence_idx < block.num_sequences, 'index is {} but size is {}'.format(sequence_idx, self.seq_pre_block_buf[block_idx])\n",
    "\n",
    "                burn_in_step = block.burn_in_steps[sequence_idx]\n",
    "                learning_step = block.learning_steps[sequence_idx]\n",
    "                forward_step = block.forward_steps[sequence_idx]\n",
    "                \n",
    "                start_idx = block.burn_in_steps[0] + np.sum(block.learning_steps[:sequence_idx])\n",
    "\n",
    "                obs = block.obs[start_idx-burn_in_step:start_idx+learning_step+forward_step]\n",
    "                last_action = block.last_action[start_idx-burn_in_step:start_idx+learning_step+forward_step]\n",
    "                last_reward = block.last_reward[start_idx-burn_in_step:start_idx+learning_step+forward_step]\n",
    "                obs, last_action, last_reward = torch.from_numpy(obs), torch.from_numpy(last_action), torch.from_numpy(last_reward)\n",
    "                \n",
    "                start_idx = np.sum(block.learning_steps[:sequence_idx])\n",
    "                end_idx = start_idx + block.learning_steps[sequence_idx]\n",
    "                action = block.action[start_idx:end_idx]\n",
    "                reward = block.n_step_reward[start_idx:end_idx]\n",
    "                gamma = block.gamma[start_idx:end_idx]\n",
    "                hidden = block.hidden[sequence_idx]\n",
    "                \n",
    "                batch_obs.append(obs)\n",
    "                batch_last_action.append(last_action)\n",
    "                batch_last_reward.append(last_reward)\n",
    "                batch_action.append(action)\n",
    "                batch_reward.append(reward)\n",
    "                batch_gamma.append(gamma)\n",
    "                batch_hidden.append(hidden)\n",
    "\n",
    "                burn_in_steps.append(burn_in_step)\n",
    "                learning_steps.append(learning_step)\n",
    "                forward_steps.append(forward_step)\n",
    "\n",
    "            batch_obs = pad_sequence(batch_obs, batch_first=True)\n",
    "            batch_last_action = pad_sequence(batch_last_action, batch_first=True)\n",
    "            batch_last_reward = pad_sequence(batch_last_reward, batch_first=True)\n",
    "\n",
    "            is_weights = np.repeat(is_weights, learning_steps)\n",
    "\n",
    "            data = (\n",
    "                batch_obs,\n",
    "                batch_last_action,\n",
    "                batch_last_reward,\n",
    "                torch.from_numpy(np.stack(batch_hidden)).transpose(0, 1),\n",
    "\n",
    "                torch.from_numpy(np.concatenate(batch_action)).unsqueeze(1),\n",
    "                torch.from_numpy(np.concatenate(batch_reward)),\n",
    "                torch.from_numpy(np.concatenate(batch_gamma)),\n",
    "\n",
    "                torch.ByteTensor(burn_in_steps),\n",
    "                torch.ByteTensor(learning_steps),\n",
    "                torch.ByteTensor(forward_steps),\n",
    "\n",
    "                idxes,\n",
    "                torch.from_numpy(is_weights.astype(np.float32)),\n",
    "                self.block_ptr,\n",
    "\n",
    "                self.env_steps\n",
    "            )\n",
    "\n",
    "        return data\n",
    "\n",
    "    def update_priorities(self, idxes: np.ndarray, td_errors: np.ndarray, old_ptr: int, loss: float):\n",
    "        \"\"\"Update priorities of sampled transitions\"\"\"\n",
    "        with self.lock:\n",
    "\n",
    "            # discard the idxes that already been replaced by new data in replay buffer during training\n",
    "            if self.block_ptr > old_ptr:\n",
    "                # range from [old_ptr, self.seq_ptr)\n",
    "                mask = (idxes < old_ptr*self.seq_pre_block) | (idxes >= self.block_ptr*self.seq_pre_block)\n",
    "                idxes = idxes[mask]\n",
    "                td_errors = td_errors[mask]\n",
    "            elif self.block_ptr < old_ptr:\n",
    "                # range from [0, self.seq_ptr) & [old_ptr, self,capacity)\n",
    "                mask = (idxes < old_ptr*self.seq_pre_block) & (idxes >= self.block_ptr*self.seq_pre_block)\n",
    "                idxes = idxes[mask]\n",
    "                td_errors = td_errors[mask]\n",
    "\n",
    "            self.priority_tree.update(idxes, td_errors)\n",
    "\n",
    "        self.training_steps += 1\n",
    "        self.sum_loss += loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18211db5539ae96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentState:\n",
    "    obs: torch.tensor\n",
    "    action_dim: int\n",
    "    last_action: torch.tensor = field(init=False)\n",
    "    last_reward: torch.tensor = torch.zeros((1, 1), dtype=torch.float32)\n",
    "    hidden_state: tuple[torch.tensor, torch.tensor] | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.last_action = torch.zeros((1, self.action_dim), dtype=torch.float32)\n",
    "\n",
    "    def update(self, obs, last_action, last_reward, hidden):\n",
    "        self.obs = torch.from_numpy(obs).unsqueeze(0)\n",
    "        self.last_action = torch.tensor([[1 if i == last_action else 0 for i in range(self.action_dim)]],\n",
    "                                        dtype=torch.float32)\n",
    "        self.last_reward = torch.tensor([[last_reward]], dtype=torch.float32)\n",
    "        self.hidden_state = hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec4addf644e7b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:46:31.204482Z",
     "start_time": "2024-05-14T09:46:31.200932Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, observation_dim, action_dim, hidden_dim=hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.obs_shape = (observation_dim,)\n",
    "        self.max_forward_steps = 5\n",
    "\n",
    "        self.recurrent = nn.LSTM(observation_dim + action_dim + 1, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: AgentState):\n",
    "        recurrent_input = torch.cat([state.obs, state.last_action, state.last_reward], dim=1)\n",
    "        \n",
    "        _, recurrent_output = self.recurrent(recurrent_input, state.hidden_state)\n",
    "        \n",
    "        hidden = recurrent_output[0]\n",
    "        \n",
    "        adv = self.advantage(hidden)\n",
    "        val = self.value(hidden)\n",
    "        q_value = val + adv - adv.mean(1, keepdim=True)\n",
    "        \n",
    "        return q_value, recurrent_output\n",
    "\n",
    "    def calculate_q_(self, obs, last_action, last_reward, hidden_state, burn_in_steps, learning_steps, forward_steps):\n",
    "        # obs shape: (batch_size, seq_len, obs_shape)\n",
    "        batch_size, max_seq_len, *_ = obs.size()\n",
    "\n",
    "        obs = obs.reshape(-1, *self.obs_shape)\n",
    "        last_action = last_action.view(-1, self.action_dim)\n",
    "        last_reward = last_reward.view(-1, 1)\n",
    "        # latent = self.feature(obs)\n",
    "\n",
    "        seq_len = burn_in_steps + learning_steps + forward_steps\n",
    "\n",
    "        recurrent_input = torch.cat((obs, last_action, last_reward), dim=1)\n",
    "        recurrent_input = recurrent_input.view(batch_size, max_seq_len, -1)\n",
    "\n",
    "        recurrent_input = pack_padded_sequence(recurrent_input, seq_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        self.recurrent.flatten_parameters()\n",
    "        recurrent_output, _ = self.recurrent(recurrent_input, hidden_state)\n",
    "\n",
    "        recurrent_output, _ = pad_packed_sequence(recurrent_output, batch_first=True)\n",
    "\n",
    "        seq_start_idx = burn_in_steps + self.max_forward_steps\n",
    "        forward_pad_steps = torch.minimum(self.max_forward_steps - forward_steps, learning_steps)\n",
    "\n",
    "        hidden = []\n",
    "        for hidden_seq, start_idx, end_idx, padding_length in zip(recurrent_output, seq_start_idx, seq_len, forward_pad_steps):\n",
    "            hidden.append(hidden_seq[start_idx:end_idx])\n",
    "            if padding_length > 0:\n",
    "                hidden.append(hidden_seq[end_idx-1:end_idx].repeat(padding_length, 1))\n",
    "\n",
    "        hidden = torch.cat(hidden)\n",
    "\n",
    "        assert hidden.size(0) == torch.sum(learning_steps)\n",
    "\n",
    "        adv = self.advantage(hidden)\n",
    "        val = self.value(hidden)\n",
    "        q_value = val + adv - adv.mean(1, keepdim=True)\n",
    "\n",
    "        return q_value\n",
    "\n",
    "    def calculate_q(self, obs, last_action, last_reward, hidden_state, burn_in_steps, learning_steps):\n",
    "        # obs shape: (batch_size, seq_len, obs_shape)\n",
    "        batch_size, max_seq_len, *_ = obs.size()\n",
    "\n",
    "        obs = obs.reshape(-1, *self.obs_shape)\n",
    "        last_action = last_action.view(-1, self.action_dim)\n",
    "        last_reward = last_reward.view(-1, 1)\n",
    "\n",
    "        # latent = self.feature(obs)\n",
    "\n",
    "        seq_len = burn_in_steps + learning_steps\n",
    "\n",
    "        recurrent_input = torch.cat((obs, last_action, last_reward), dim=1)\n",
    "        recurrent_input = recurrent_input.view(batch_size, max_seq_len, -1)\n",
    "        recurrent_input = pack_padded_sequence(recurrent_input, seq_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # self.recurrent.flatten_parameters()\n",
    "        recurrent_output, _ = self.recurrent(recurrent_input, hidden_state)\n",
    "\n",
    "        recurrent_output, _ = pad_packed_sequence(recurrent_output, batch_first=True)\n",
    "\n",
    "        hidden = torch.cat([output[burn_in:burn_in+learning] for output, burn_in, learning in zip(recurrent_output, burn_in_steps, learning_steps)], dim=0)\n",
    "\n",
    "        adv = self.advantage(hidden)\n",
    "        val = self.value(hidden)\n",
    "\n",
    "        q_value = val + adv - adv.mean(1, keepdim=True)\n",
    "\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0029246daacd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mixed_td_errors(td_error, learning_steps):\n",
    "    \n",
    "    start_idx = 0\n",
    "    mixed_td_errors = np.empty(learning_steps.shape, dtype=td_error.dtype)\n",
    "    for i, steps in enumerate(learning_steps):\n",
    "        mixed_td_errors[i] = 0.9*td_error[start_idx:start_idx+steps].max() + 0.1*td_error[start_idx:start_idx+steps].mean()\n",
    "        start_idx += steps\n",
    "    \n",
    "    return mixed_td_errors\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, batch_queue, priority_queue, model, grad_norm: int = grad_norm,\n",
    "                lr: float = lr, eps:float = eps, game_name: str = game_name,\n",
    "                target_net_update_interval: int = target_net_update_interval, save_interval: int = save_interval):\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.online_net = deepcopy(model)\n",
    "        self.online_net.to(self.device)\n",
    "        self.online_net.train()\n",
    "        self.target_net = deepcopy(self.online_net)\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=lr, eps=eps)\n",
    "        self.loss_fn = nn.MSELoss(reduction='none')\n",
    "        self.grad_norm = grad_norm\n",
    "        self.batch_queue = batch_queue\n",
    "        self.priority_queue = priority_queue\n",
    "        self.num_updates = 0\n",
    "        self.done = False\n",
    "\n",
    "        self.target_net_update_interval = target_net_update_interval\n",
    "        self.save_interval = save_interval\n",
    "\n",
    "        self.batched_data = []\n",
    "\n",
    "        self.shared_model = model\n",
    "\n",
    "        self.game_name = game_name\n",
    "\n",
    "    def store_weights(self):\n",
    "        self.shared_model.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        while True:\n",
    "            if not self.batch_queue.empty() and len(self.batched_data) < 4:\n",
    "                data = self.batch_queue.get_nowait()\n",
    "                self.batched_data.append(data)\n",
    "            else:\n",
    "                time.sleep(0.1)\n",
    "\n",
    "    def run(self):\n",
    "        background_thread = threading.Thread(target=self.prepare_data, daemon=True)\n",
    "        background_thread.start()\n",
    "        time.sleep(2)\n",
    "\n",
    "        start_time = time.time()\n",
    "        while self.num_updates < training_steps:\n",
    "            \n",
    "            while not self.batched_data:\n",
    "                time.sleep(1)\n",
    "            data = self.batched_data.pop(0)\n",
    "\n",
    "            batch_obs, batch_last_action, batch_last_reward, batch_hidden, batch_action, batch_n_step_reward, batch_n_step_gamma, burn_in_steps, learning_steps, forward_steps, idxes, is_weights, old_ptr, env_steps = data\n",
    "            batch_obs, batch_last_action, batch_last_reward = batch_obs.to(self.device), batch_last_action.to(self.device), batch_last_reward.to(self.device)\n",
    "            batch_hidden, batch_action = batch_hidden.to(self.device), batch_action.to(self.device)\n",
    "            batch_n_step_reward, batch_n_step_gamma = batch_n_step_reward.to(self.device), batch_n_step_gamma.to(self.device)\n",
    "            is_weights = is_weights.to(self.device)\n",
    "\n",
    "            batch_obs, batch_last_action = batch_obs.float(), batch_last_action.float()\n",
    "            batch_action = batch_action.long()\n",
    "            burn_in_steps, learning_steps, forward_steps = burn_in_steps, learning_steps, forward_steps\n",
    "\n",
    "            batch_hidden = (batch_hidden[:1], batch_hidden[1:])\n",
    "\n",
    "            # batch_obs = batch_obs / 255\n",
    "\n",
    "            # double q learning\n",
    "            with torch.no_grad():\n",
    "                batch_action_ = self.online_net.calculate_q_(batch_obs, batch_last_action, batch_last_reward, batch_hidden, burn_in_steps, learning_steps, forward_steps).argmax(1).unsqueeze(1)\n",
    "                batch_q_ = self.target_net.calculate_q_(batch_obs, batch_last_action, batch_last_reward, batch_hidden, burn_in_steps, learning_steps, forward_steps).gather(1, batch_action_).squeeze(1)\n",
    "            \n",
    "            target_q = self.value_rescale(batch_n_step_reward + batch_n_step_gamma * self.inverse_value_rescale(batch_q_))\n",
    "            # target_q = batch_n_step_reward + batch_n_step_gamma * batch_q_\n",
    "\n",
    "            batch_q = self.online_net.calculate_q(batch_obs, batch_last_action, batch_last_reward, batch_hidden, burn_in_steps, learning_steps).gather(1, batch_action).squeeze(1)\n",
    "            \n",
    "            loss = (is_weights * self.loss_fn(batch_q, target_q)).mean()\n",
    "\n",
    "            \n",
    "            td_errors = (target_q-batch_q).detach().clone().squeeze().abs().cpu().float().numpy()\n",
    "\n",
    "            priorities = calculate_mixed_td_errors(td_errors, learning_steps.numpy())\n",
    "\n",
    "            # automatic mixed precision training\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.online_net.parameters(), self.grad_norm)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.num_updates += 1\n",
    "\n",
    "            self.priority_queue.put((idxes, priorities, old_ptr, loss.item()))\n",
    "\n",
    "            # store new weights in shared memory\n",
    "            if self.num_updates % 4 == 0:\n",
    "                self.store_weights()\n",
    "\n",
    "            # update target net\n",
    "            if self.num_updates % self.target_net_update_interval == 0:\n",
    "                self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "            \n",
    "            # save model \n",
    "            if self.num_updates % self.save_interval == 0:\n",
    "                os.makedirs('models/r2d2', exist_ok=True)\n",
    "                torch.save((self.online_net.state_dict(), self.num_updates, env_steps, (time.time()-start_time)/60), os.path.join('models/r2d2', '{}{}.pth'.format(self.game_name, self.num_updates)))\n",
    "\n",
    "    @staticmethod\n",
    "    def value_rescale(value, eps=1e-3):\n",
    "        return value.sign()*((value.abs()+1).sqrt()-1) + eps*value\n",
    "\n",
    "    @staticmethod\n",
    "    def inverse_value_rescale(value, eps=1e-3):\n",
    "        temp = ((1 + 4*eps*(value.abs()+1+eps)).sqrt() - 1) / (2*eps)\n",
    "        return value.sign() * (temp.square() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e982c17b637a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBuffer:\n",
    "    def __init__(self, action_dim: int, forward_steps: int = forward_steps,\n",
    "                burn_in_steps = burn_in_steps, learning_steps: int = learning_steps, \n",
    "                gamma: float = gamma, hidden_dim: int = hidden_dim, block_length: int = block_length):\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.forward_steps = forward_steps\n",
    "        self.learning_steps = learning_steps\n",
    "        self.burn_in_steps = burn_in_steps\n",
    "        self.block_length = block_length\n",
    "        self.curr_burn_in_steps = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def reset(self, init_obs: np.ndarray):\n",
    "        self.obs_buffer = [init_obs]\n",
    "        self.last_action_buffer = [np.array([1 if i == 0 else 0 for i in range(self.action_dim)], dtype=bool)]\n",
    "        self.last_reward_buffer = [0]\n",
    "        self.hidden_buffer = [np.zeros((2, self.hidden_dim), dtype=np.float32)]\n",
    "        self.action_buffer = []\n",
    "        self.reward_buffer = []\n",
    "        self.qval_buffer = []\n",
    "        self.curr_burn_in_steps = 0\n",
    "        self.size = 0\n",
    "        self.sum_reward = 0\n",
    "        self.done = False\n",
    "\n",
    "    def add(self, action: int, reward: float, next_obs: np.ndarray, q_value: np.ndarray, hidden_state: np.ndarray):\n",
    "        self.action_buffer.append(action)\n",
    "        self.reward_buffer.append(reward)\n",
    "        self.hidden_buffer.append(hidden_state)\n",
    "        self.obs_buffer.append(next_obs)\n",
    "        self.last_action_buffer.append(np.array([1 if i == action else 0 for i in range(self.action_dim)], dtype=bool))\n",
    "        self.last_reward_buffer.append(reward)\n",
    "        self.qval_buffer.append(q_value)\n",
    "        self.sum_reward += reward\n",
    "        self.size += 1\n",
    "    \n",
    "    def finish(self, last_qval: np.ndarray = None) -> Tuple:\n",
    "        assert self.size <= self.block_length\n",
    "        # assert len(self.last_action_buffer) == self.curr_burn_in_steps + self.size + 1\n",
    "\n",
    "        num_sequences = math.ceil(self.size/self.learning_steps)\n",
    "\n",
    "        max_forward_steps = min(self.size, self.forward_steps)\n",
    "        n_step_gamma = [self.gamma**self.forward_steps] * (self.size-max_forward_steps)\n",
    "\n",
    "        # last_qval is none means episode done \n",
    "        if last_qval is not None:\n",
    "            self.qval_buffer.append(last_qval)\n",
    "            n_step_gamma.extend([self.gamma**i for i in reversed(range(1, max_forward_steps+1))])\n",
    "        else:\n",
    "            self.done = True\n",
    "            self.qval_buffer.append(np.zeros_like(self.qval_buffer[0]))\n",
    "            n_step_gamma.extend([0 for _ in range(max_forward_steps)]) # set gamma to 0 so don't need 'done'\n",
    "\n",
    "        n_step_gamma = np.array(n_step_gamma, dtype=np.float32)\n",
    "\n",
    "        obs = np.stack(self.obs_buffer)\n",
    "        last_action = np.stack(self.last_action_buffer)\n",
    "        last_reward = np.array(self.last_reward_buffer, dtype=np.float32)\n",
    "\n",
    "        hiddens = np.stack(self.hidden_buffer[slice(0, self.size, self.learning_steps)])\n",
    "\n",
    "        actions = np.array(self.action_buffer, dtype=np.uint8)\n",
    "\n",
    "        qval_buffer = np.concatenate(self.qval_buffer)\n",
    "        reward_buffer = self.reward_buffer + [0 for _ in range(self.forward_steps-1)]\n",
    "        n_step_reward = np.convolve(reward_buffer, \n",
    "                                    [self.gamma**(self.forward_steps-1-i) for i in range(self.forward_steps)],\n",
    "                                    'valid').astype(np.float32)\n",
    "\n",
    "        burn_in_steps = np.array([min(i*self.learning_steps+self.curr_burn_in_steps, self.burn_in_steps) for i in range(num_sequences)], dtype=np.uint8)\n",
    "        learning_steps = np.array([min(self.learning_steps, self.size-i*self.learning_steps) for i in range(num_sequences)], dtype=np.uint8)\n",
    "        forward_steps = np.array([min(self.forward_steps, self.size+1-np.sum(learning_steps[:i+1])) for i in range(num_sequences)], dtype=np.uint8)\n",
    "        assert forward_steps[-1] == 1 and burn_in_steps[0] == self.curr_burn_in_steps\n",
    "        # assert last_action.shape[0] == self.curr_burn_in_steps + np.sum(learning_steps) + 1\n",
    "\n",
    "        max_qval = np.max(qval_buffer[max_forward_steps:self.size+1], axis=1)\n",
    "        max_qval = np.pad(max_qval, (0, max_forward_steps-1), 'edge')\n",
    "        target_qval = qval_buffer[np.arange(self.size), actions]\n",
    "\n",
    "        td_errors = np.abs(n_step_reward + n_step_gamma * max_qval - target_qval, dtype=np.float32)\n",
    "        priorities = np.zeros(self.block_length//self.learning_steps, dtype=np.float32)\n",
    "        priorities[:num_sequences] = calculate_mixed_td_errors(td_errors, learning_steps)\n",
    "\n",
    "        # save burn in information for next block\n",
    "        self.obs_buffer = self.obs_buffer[-self.burn_in_steps-1:]\n",
    "        self.last_action_buffer = self.last_action_buffer[-self.burn_in_steps-1:]\n",
    "        self.last_reward_buffer = self.last_reward_buffer[-self.burn_in_steps-1:]\n",
    "        self.hidden_buffer = self.hidden_buffer[-self.burn_in_steps-1:]\n",
    "        self.action_buffer.clear()\n",
    "        self.reward_buffer.clear()\n",
    "        self.qval_buffer.clear()\n",
    "        self.curr_burn_in_steps = len(self.obs_buffer)-1\n",
    "        self.size = 0\n",
    "        \n",
    "        block = Block(obs, last_action, last_reward, actions, n_step_reward, n_step_gamma, hiddens, num_sequences, burn_in_steps, learning_steps, forward_steps)\n",
    "        return [block, priorities, self.sum_reward if self.done else None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de71841dc9e844c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, epsilon: float, model, sample_queue,\n",
    "                max_episode_steps: int = max_episode_steps, block_length: int = block_length):\n",
    "\n",
    "        self.env = gym.make(game_name)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.model = Network(self.env.observation_space.shape[0], self.action_dim)\n",
    "        self.model.eval()\n",
    "        self.local_buffer = LocalBuffer(self.action_dim)\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.shared_model = model\n",
    "        self.sample_queue = sample_queue\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.block_length = block_length\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        actor_steps = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            done = False\n",
    "            agent_state = self.reset()\n",
    "            episode_steps = 0\n",
    "\n",
    "            while not done and episode_steps < self.max_episode_steps:\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    q_value, hidden = self.model(agent_state)\n",
    "\n",
    "                if random.random() < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = torch.argmax(q_value, 1).item()\n",
    "\n",
    "                # apply action in env\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                agent_state.update(next_obs, action, reward, hidden)\n",
    "\n",
    "                episode_steps += 1\n",
    "                actor_steps += 1\n",
    "\n",
    "                self.local_buffer.add(action, reward, next_obs, q_value.numpy(), torch.cat(hidden).numpy())\n",
    "\n",
    "                if done:\n",
    "                    block = self.local_buffer.finish()\n",
    "                    self.sample_queue.put(block)\n",
    "\n",
    "                elif len(self.local_buffer) == self.block_length or episode_steps == self.max_episode_steps:\n",
    "                    with torch.no_grad():\n",
    "                        q_value, hidden = self.model(agent_state)\n",
    "\n",
    "                    block = self.local_buffer.finish(q_value.numpy())\n",
    "\n",
    "                    if self.epsilon > 0.01:\n",
    "                        block[2] = None\n",
    "                    self.sample_queue.put(block)\n",
    "\n",
    "                if actor_steps % actor_update_interval == 0:\n",
    "                    self.update_weights()\n",
    "\n",
    "                \n",
    "    def update_weights(self):\n",
    "        self.model.load_state_dict(self.shared_model.state_dict())\n",
    "    \n",
    "    def reset(self):\n",
    "        obs, _ = self.env.reset()\n",
    "        self.local_buffer.reset(obs)\n",
    "\n",
    "        state = AgentState(torch.from_numpy(obs).unsqueeze(0), self.action_dim)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea899f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(actor_id: int, base_eps: float = base_eps, alpha: float = alpha, num_actors: int = num_actors):\n",
    "    exponent = 1 + actor_id / (num_actors-1) * alpha\n",
    "    return base_eps**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e97fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96088dbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m queue \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mQueue()\n\u001b[1;32m      5\u001b[0m actor \u001b[38;5;241m=\u001b[39m Actor(epsilon\u001b[38;5;241m=\u001b[39mget_epsilon(\u001b[38;5;241m0\u001b[39m), model\u001b[38;5;241m=\u001b[39mNetwork(observation_dim, action_dim), sample_queue\u001b[38;5;241m=\u001b[39mqueue)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mActor.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m episode_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m episode_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_episode_steps:\n\u001b[0;32m---> 29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n",
      "File \u001b[0;32m~/.miniforge3/envs/torch/lib/python3.11/site-packages/torch/autograd/grad_mode.py:84\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniforge3/envs/torch/lib/python3.11/site-packages/torch/autograd/grad_mode.py:183\u001b[0m, in \u001b[0;36mset_grad_enabled.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mset_grad_enabled\u001b[39;00m(_DecoratorContextManager):\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Context-manager that sets gradient calculation on or off.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    ``set_grad_enabled`` will enable or disable grads based on its argument :attr:`mode`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(game_name)\n",
    "action_dim = env.action_space.n\n",
    "observation_dim = env.observation_space.shape[0]\n",
    "queue = mp.Queue()\n",
    "actor = Actor(epsilon=get_epsilon(0), model=Network(observation_dim, action_dim), sample_queue=queue)\n",
    "actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06c5b41ae86fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 221\n",
      "Buffer update speed: 36.9/s\n",
      "Number of environment steps: 400\n",
      "Average episode return: 10.5263\n",
      "Number of training steps: 0\n",
      "Training speed: 0.0/s\n",
      "\n",
      "tensor([1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 5, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 4, 1, 4, 1, 1, 1, 1, 4,\n",
      "        1, 1, 1, 2, 1, 1, 5, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 5, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 4, 1, 4, 1, 1, 1, 1, 4,\n",
      "        1, 1, 1, 2, 1, 1, 5, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 2, 1, 4, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 2, 1, 4, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 5, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 5, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 5, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 5, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 2, 1, 2, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 2, 1, 2, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 2, 2, 1,\n",
      "        2, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 2, 2, 1,\n",
      "        2, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 2, 1, 1, 5, 2, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        2, 2, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 2, 1, 1, 5, 2, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        2, 2, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 2, 1, 1, 1, 1, 2, 5, 5, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 2, 1, 1, 1, 1, 2, 5, 5, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 5, 5, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 5, 5, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
      "        1, 1, 1, 1, 3, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
      "        1, 1, 1, 1, 3, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 4, 1, 1, 1, 1, 3], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 4, 1, 1, 1, 1, 3], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 1,\n",
      "        1, 1, 1, 1, 3, 1, 2, 1], dtype=torch.uint8)\n",
      "tensor([1, 5, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 1,\n",
      "        1, 1, 1, 1, 3, 1, 2, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1,\n",
      "        3, 1, 2, 2, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1,\n",
      "        3, 1, 2, 2, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        5, 1, 1, 1, 3, 1, 1, 3], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        5, 1, 1, 1, 3, 1, 1, 3], dtype=torch.uint8)\n",
      "tensor([3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 5, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 5, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3,\n",
      "        1, 1, 1, 1, 1, 4, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3,\n",
      "        1, 1, 1, 1, 1, 4, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 1], dtype=torch.uint8)\n",
      "tensor([1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 3, 1, 2, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 3, 1, 2, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 5, 3, 1, 1, 2, 2,\n",
      "        3, 3, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 5, 3, 1, 1, 2, 2,\n",
      "        3, 3, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([2, 2, 2, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1,\n",
      "        2, 1, 1, 3, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([2, 2, 2, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1,\n",
      "        2, 1, 1, 3, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
      "        5, 1, 1, 1, 1, 2, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
      "        5, 1, 1, 1, 1, 2, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 5, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 5, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 1], dtype=torch.uint8)\n",
      "tensor([2, 1, 1, 1, 1, 1, 1, 5, 2, 3, 1, 1, 1, 1, 2, 5, 1, 1, 2, 1, 1, 1, 1, 3,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([2, 1, 1, 1, 1, 1, 1, 5, 2, 3, 1, 1, 1, 1, 2, 5, 1, 1, 2, 1, 1, 1, 1, 3,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 5, 2, 1,\n",
      "        1, 3, 1, 1, 2, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 5, 2, 1,\n",
      "        1, 3, 1, 1, 2, 1, 1, 1], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 1, 1, 1, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m buffer_proc \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mProcess(target\u001b[38;5;241m=\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mrun)\n\u001b[1;32m     26\u001b[0m buffer_proc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m buffer_proc\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proc \u001b[38;5;129;01min\u001b[39;00m actor_procs:\n",
      "Cell \u001b[0;32mIn[8], line 79\u001b[0m, in \u001b[0;36mLearner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# batch_obs = batch_obs / 255\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# double q learning\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 79\u001b[0m     batch_action_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_q_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_last_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_last_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mburn_in_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     80\u001b[0m     batch_q_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mcalculate_q_(batch_obs, batch_last_action, batch_last_reward, batch_hidden, burn_in_steps, learning_steps, forward_steps)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, batch_action_)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m target_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_rescale(batch_n_step_reward \u001b[38;5;241m+\u001b[39m batch_n_step_gamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_value_rescale(batch_q_))\n",
      "Cell \u001b[0;32mIn[7], line 65\u001b[0m, in \u001b[0;36mNetwork.calculate_q_\u001b[0;34m(self, obs, last_action, last_reward, hidden_state, burn_in_steps, learning_steps, forward_steps)\u001b[0m\n\u001b[1;32m     63\u001b[0m     hidden\u001b[38;5;241m.\u001b[39mappend(hidden_seq[start_idx:end_idx])\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padding_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m         hidden\u001b[38;5;241m.\u001b[39mappend(\u001b[43mhidden_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     67\u001b[0m hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(hidden)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hidden\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(learning_steps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size: 9864\n",
      "Buffer update speed: 946.4/s\n",
      "Number of environment steps: 13107\n",
      "Average episode return: 9.8778\n",
      "Number of training steps: 30\n",
      "Training speed: 3.0/s\n",
      "Loss: 0.9248\n",
      "\n",
      "Buffer size: 9863\n",
      "Buffer update speed: -0.1/s\n",
      "Number of environment steps: 30810\n",
      "Average episode return: 9.8092\n",
      "Number of training steps: 30\n",
      "Training speed: 0.0/s\n",
      "\n",
      "Buffer size: 9864\n",
      "Buffer update speed: -1.0/s\n",
      "Number of environment steps: 45692\n",
      "Average episode return: 9.8552\n",
      "Number of training steps: 30\n",
      "Training speed: 0.0/s\n",
      "\n",
      "Buffer size: 9722\n",
      "Buffer update speed: -12.6/s\n",
      "Number of environment steps: 60712\n",
      "Average episode return: 9.7584\n",
      "Number of training steps: 30\n",
      "Training speed: 0.0/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "env = gym.make(game_name)\n",
    "n_observations = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "model = Network(n_observations, n_actions)\n",
    "model.share_memory()\n",
    "sample_queue_list = [mp.Queue() for _ in range(num_actors)]\n",
    "batch_queue = mp.Queue(num_actors)\n",
    "priority_queue = mp.Queue(num_actors)\n",
    "\n",
    "buffer = ReplayBuffer(sample_queue_list, batch_queue, priority_queue)\n",
    "learner = Learner(batch_queue, priority_queue, model)\n",
    "actors = [Actor(get_epsilon(i), model, sample_queue_list[i]) for i in range(num_actors)]\n",
    "\n",
    "actor_procs = [mp.Process(target=actor.run) for actor in actors]\n",
    "for proc in actor_procs:\n",
    "    proc.start()\n",
    "\n",
    "buffer_proc = mp.Process(target=buffer.run)\n",
    "buffer_proc.start()\n",
    "\n",
    "learner.run()\n",
    "\n",
    "buffer_proc.join()\n",
    "\n",
    "for proc in actor_procs:\n",
    "    proc.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dce6a1",
   "metadata": {},
   "source": [
    "### 22 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45d632",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3264dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(game_name, render_mode='human')\n",
    "obs, _ = env.reset()\n",
    "state = AgentState(torch.from_numpy(obs).unsqueeze(0), n_actions)\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        q_value, hidden = model(state)\n",
    "    action = torch.argmax(q_value, 1).item()\n",
    "    obs, reward, terminated, truncated, _, = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    state.update(obs, action, reward, hidden)\n",
    "env.close()\n",
    "print(total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
